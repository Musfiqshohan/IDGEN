{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T19:07:53.406956Z",
     "start_time": "2024-10-25T19:07:53.402233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "2+2"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:50:48.729352Z",
     "start_time": "2024-10-25T20:50:48.724896Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np\n",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:50:52.504311Z",
     "start_time": "2024-10-25T20:50:52.396842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyAgrum as gum\n",
    "import pyAgrum.lib.notebook as gnb\n",
    "\n",
    "print('imported')\n",
    "\n",
    "bn=gum.BayesNet('backdoor')\n",
    "Z,X,Y=[ bn.add(name, 2) for name in \"ZXY\" ] \n",
    "print (bn)\n",
    "\n",
    "all_nodes=['X','Z', 'Y'] \n",
    "\n",
    "bn.addArc(Z,Y)\n",
    "bn.addArc(Z,X)\n",
    "bn.addArc(X,Y)\n",
    "gnb.flow.row(bn)\n",
    "\n",
    "bn"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "BN{nodes: 3, arcs: 0, domainSize: 8, dim: 3, mem: 48o}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <style>\n",
       "      .floating-box {\n",
       "      display: inline-block;\n",
       "      margin: 7px;\n",
       "      padding : 3px;\n",
       "      border: 0px solid transparent;  \n",
       "      valign:middle;\n",
       "      background-color: transparent;\n",
       "      }\n",
       "      </style>\n",
       "      <div class=\"floating-box\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"89pt\" height=\"188pt\" viewBox=\"0.00 0.00 89.00 188.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>G</title>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"(1) X\">\n",
       "<ellipse fill=\"#404040\" stroke=\"#4a4a4a\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">X</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<g id=\"a_node2\"><a xlink:title=\"(2) Y\">\n",
       "<ellipse fill=\"#404040\" stroke=\"#4a4a4a\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Y</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- X&#45;&gt;Y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X-&gt;Y</title>\n",
       "<g id=\"a_edge3\"><a xlink:title=\"1 → 2\">\n",
       "<path fill=\"none\" stroke=\"#4a4a4a\" d=\"M33.4,-72.41C36.51,-64.34 40.33,-54.43 43.83,-45.35\"/>\n",
       "<polygon fill=\"#4a4a4a\" stroke=\"#4a4a4a\" points=\"47.13,-46.55 47.46,-35.96 40.6,-44.03 47.13,-46.55\"/>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Z -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Z</title>\n",
       "<g id=\"a_node3\"><a xlink:title=\"(0) Z\">\n",
       "<ellipse fill=\"#404040\" stroke=\"#4a4a4a\" cx=\"54\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Z</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Z&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Z-&gt;X</title>\n",
       "<g id=\"a_edge1\"><a xlink:title=\"0 → 1\">\n",
       "<path fill=\"none\" stroke=\"#4a4a4a\" d=\"M47.6,-144.41C44.49,-136.34 40.67,-126.43 37.17,-117.35\"/>\n",
       "<polygon fill=\"#4a4a4a\" stroke=\"#4a4a4a\" points=\"40.4,-116.03 33.54,-107.96 33.87,-118.55 40.4,-116.03\"/>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Z&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Z-&gt;Y</title>\n",
       "<g id=\"a_edge2\"><a xlink:title=\"0 → 2\">\n",
       "<path fill=\"none\" stroke=\"#4a4a4a\" d=\"M57.65,-143.91C59.68,-133.57 61.98,-120.09 63,-108 64.34,-92.06 64.34,-87.94 63,-72 62.28,-63.5 60.93,-54.31 59.49,-46.01\"/>\n",
       "<polygon fill=\"#4a4a4a\" stroke=\"#4a4a4a\" points=\"62.91,-45.29 57.65,-36.09 56.03,-46.56 62.91,-45.29\"/>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "</g>\n",
       "</svg></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(pyAgrum.BayesNet<double>@0x561dd0ae1760) BN{nodes: 3, arcs: 3, domainSize: 8, dim: 7, mem: 112o}"
      ],
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"89pt\" height=\"188pt\" viewBox=\"0.00 0.00 89.00 188.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>G</title>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"(1) X\">\n",
       "<ellipse fill=\"#404040\" stroke=\"#4a4a4a\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">X</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<g id=\"a_node2\"><a xlink:title=\"(2) Y\">\n",
       "<ellipse fill=\"#404040\" stroke=\"#4a4a4a\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Y</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- X&#45;&gt;Y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X-&gt;Y</title>\n",
       "<g id=\"a_edge3\"><a xlink:title=\"1 → 2\">\n",
       "<path fill=\"none\" stroke=\"#4a4a4a\" d=\"M33.4,-72.41C36.51,-64.34 40.33,-54.43 43.83,-45.35\"/>\n",
       "<polygon fill=\"#4a4a4a\" stroke=\"#4a4a4a\" points=\"47.13,-46.55 47.46,-35.96 40.6,-44.03 47.13,-46.55\"/>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Z -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Z</title>\n",
       "<g id=\"a_node3\"><a xlink:title=\"(0) Z\">\n",
       "<ellipse fill=\"#404040\" stroke=\"#4a4a4a\" cx=\"54\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Z</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Z&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Z-&gt;X</title>\n",
       "<g id=\"a_edge1\"><a xlink:title=\"0 → 1\">\n",
       "<path fill=\"none\" stroke=\"#4a4a4a\" d=\"M47.6,-144.41C44.49,-136.34 40.67,-126.43 37.17,-117.35\"/>\n",
       "<polygon fill=\"#4a4a4a\" stroke=\"#4a4a4a\" points=\"40.4,-116.03 33.54,-107.96 33.87,-118.55 40.4,-116.03\"/>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Z&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Z-&gt;Y</title>\n",
       "<g id=\"a_edge2\"><a xlink:title=\"0 → 2\">\n",
       "<path fill=\"none\" stroke=\"#4a4a4a\" d=\"M57.65,-143.91C59.68,-133.57 61.98,-120.09 63,-108 64.34,-92.06 64.34,-87.94 63,-72 62.28,-63.5 60.93,-54.31 59.49,-46.01\"/>\n",
       "<polygon fill=\"#4a4a4a\" stroke=\"#4a4a4a\" points=\"62.91,-45.29 57.65,-36.09 56.03,-46.56 62.91,-45.29\"/>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:50:56.795106Z",
     "start_time": "2024-10-25T20:50:56.792841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for nd in all_nodes:\n",
    "    bn.generateCPT(nd)\n",
    "    print(bn.cpt(nd))\n",
    "    # print(nd)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:50:58.828382Z",
     "start_time": "2024-10-25T20:50:58.823041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ie=gum.LazyPropagation(bn)\n",
    "ie.makeInference()\n",
    "\n",
    "# ie.evidenceJointImpact(['Z','X'], [])\n",
    "ie.evidenceJointImpact(['Z','X', 'Y'], [])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyAgrum.Potential<double>@0x561dce6a7d90) \n",
       "             ||  X                |\n",
       "Z     |Y     ||0        |1        |\n",
       "------|------||---------|---------|\n",
       "0     |0     || 0.1019  | 0.0001  |\n",
       "1     |0     || 0.0543  | 0.0059  |\n",
       "0     |1     || 0.2773  | 0.0086  |\n",
       "1     |1     || 0.1394  | 0.4125  |"
      ],
      "text/html": [
       "<table style=\"border:1px solid black;border-collapse: collapse;\">\n",
       "<tr><th colspan='2'></th>\n",
       "      <th colspan='2' style='border:1px solid black;color:black;background-color:#808080;'><center>X</center>\n",
       "      </th></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#808080'><center>Y</center></th><th style='border:1px solid black;color:black;background-color:#808080'><center>Z</center></th><th style='border:1px solid black;border-bottom-style: double;color:black;background-color:#BBBBBB'>\n",
       "      <center>0</center></th><th style='border:1px solid black;border-bottom-style: double;color:black;background-color:#BBBBBB'>\n",
       "      <center>1</center></th></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#BBBBBB;' rowspan = '2'>\n",
       "            <center>0</center></th><th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>0</center></th><td style='color:black;background-color:#f18c64;text-align:right;padding: 3px;'>0.1019</td><td style='color:black;background-color:#fe7f64;text-align:right;padding: 3px;'>0.0001</td></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>1</center></th><td style='color:black;background-color:#f88564;text-align:right;padding: 3px;'>0.0543</td><td style='color:black;background-color:#fe7f64;text-align:right;padding: 3px;'>0.0059</td></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#BBBBBB;' rowspan = '2'>\n",
       "            <center>1</center></th><th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>0</center></th><td style='color:black;background-color:#dba264;text-align:right;padding: 3px;'>0.2773</td><td style='color:black;background-color:#fd8064;text-align:right;padding: 3px;'>0.0086</td></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>1</center></th><td style='color:black;background-color:#ed9064;text-align:right;padding: 3px;'>0.1394</td><td style='color:black;background-color:#cab364;text-align:right;padding: 3px;'>0.4125</td></tr>\n",
       "</table>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:50:59.532216Z",
     "start_time": "2024-10-25T20:50:59.527273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ie=gum.LazyPropagation(bn)\n",
    "ie.makeInference()\n",
    "\n",
    "ie.evidenceJointImpact(['X'], [])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyAgrum.Potential<double>@0x561dd0c14eb0) \n",
       "  X                |\n",
       "0        |1        |\n",
       "---------|---------|\n",
       " 0.5728  | 0.4272  |"
      ],
      "text/html": [
       "<table style=\"border:1px solid black;border-collapse: collapse;\">\n",
       "<tr style='border:1px solid black;color:black;background-color:#808080'>\n",
       "      <th colspan='2'><center>X</center></th></tr>\n",
       "<tr><th style='border:1px solid black;border-bottom-style: double;color:black;background-color:#BBBBBB'>\n",
       "      <center>0</center></th><th style='border:1px solid black;border-bottom-style: double;color:black;background-color:#BBBBBB'>\n",
       "      <center>1</center></th></tr>\n",
       "<tr><td style='color:black;background-color:#b5c864;text-align:right;padding: 3px;'>0.5728</td><td style='color:black;background-color:#c8b564;text-align:right;padding: 3px;'>0.4272</td></tr>\n",
       "</table>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Causal Effect"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:09.348209Z",
     "start_time": "2024-10-25T20:51:09.343310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out= bn.cpt('Z')* bn.cpt('Y')\n",
    "# out\n",
    "true_doX_cpt = out.margSumOut('Z')\n",
    "true_doX_cpt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyAgrum.Potential<double>@0x561dce9c54a0) \n",
       "      ||  Y                |\n",
       "X     ||0        |1        |\n",
       "------||---------|---------|\n",
       "0     || 0.2757  | 0.7243  |\n",
       "1     || 0.0134  | 0.9866  |"
      ],
      "text/html": [
       "<table style=\"border:1px solid black;border-collapse: collapse;\">\n",
       "<tr><th colspan='1'></th>\n",
       "      <th colspan='2' style='border:1px solid black;color:black;background-color:#808080;'><center>Y</center>\n",
       "      </th></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#808080'><center>X</center></th><th style='border:1px solid black;border-bottom-style: double;color:black;background-color:#BBBBBB'>\n",
       "      <center>0</center></th><th style='border:1px solid black;border-bottom-style: double;color:black;background-color:#BBBBBB'>\n",
       "      <center>1</center></th></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>0</center></th><td style='color:black;background-color:#dba264;text-align:right;padding: 3px;'>0.2757</td><td style='color:black;background-color:#a2db64;text-align:right;padding: 3px;'>0.7243</td></tr>\n",
       "<tr><th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>1</center></th><td style='color:black;background-color:#fd8064;text-align:right;padding: 3px;'>0.0134</td><td style='color:black;background-color:#80fd64;text-align:right;padding: 3px;'>0.9866</td></tr>\n",
       "</table>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:10.132445Z",
     "start_time": "2024-10-25T20:51:10.127672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_doX={}\n",
    "for x in true_doX_cpt.loopIn():\n",
    "    # print(x.todict(), true_doX_cpt[x])\n",
    "    true_doX[tuple(x.todict().values())]= true_doX_cpt[x]\n",
    "\n",
    "true_doX"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.27568764613056196,\n",
       " (1, 0): 0.7243123538694383,\n",
       " (0, 1): 0.013366143088356817,\n",
       " (1, 1): 0.9866338569116433}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data generation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:11.216669Z",
     "start_time": "2024-10-25T20:51:11.169656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g=gum.BNDatabaseGenerator(bn)\n",
    "N= 10000\n",
    "g.drawSamples(N,{})\n",
    "dataset= g.to_pandas()"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:11.591307Z",
     "start_time": "2024-10-25T20:51:11.570495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "dataset = np.array(dataset).astype(int)\n",
    "type(dataset)\n",
    "unique_rows, counts = np.unique(dataset, axis=0, return_counts=True)\n",
    "joint_distribution = counts / len(dataset)\n",
    "\n",
    "# Print the unique rows and their corresponding probabilities\n",
    "for row, prob in zip(unique_rows, joint_distribution):\n",
    "    print(f\"Row: {row}, Probability: {prob:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: [0 0 0], Probability: 0.1059\n",
      "Row: [0 0 1], Probability: 0.2714\n",
      "Row: [0 1 0], Probability: 0.0004\n",
      "Row: [0 1 1], Probability: 0.0082\n",
      "Row: [1 0 0], Probability: 0.0523\n",
      "Row: [1 0 1], Probability: 0.1381\n",
      "Row: [1 1 0], Probability: 0.0067\n",
      "Row: [1 1 1], Probability: 0.4170\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:11.874944Z",
     "start_time": "2024-10-25T20:51:11.870188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# N=10000\n",
    "# sampleZ = np.random.binomial(n=1, p=pz[id,1], size=N)\n",
    "# sampleX_Z = np.random.binomial(n=1, p=px_z[id,1], size=N)\n",
    "# sampleY_XZ = np.random.binomial(n=1, p=py_xz[id,1], size=N)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "sampleZ = torch.nn.functional.one_hot(torch.tensor(dataset[:,0]), num_classes=2).float()  \n",
    "sampleX = torch.nn.functional.one_hot(torch.tensor(dataset[:,1]), num_classes=2).float()\n",
    "sampleY = torch.nn.functional.one_hot(torch.tensor(dataset[:,2]), num_classes=2).float()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training code"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:12.978979Z",
     "start_time": "2024-10-25T20:51:12.966825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature, gumbel_noise):\n",
    "\n",
    "    if gumbel_noise==None:\n",
    "        gumbel_noise= sample_gumbel(logits.size())\n",
    "\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature, gumbel_noise=None, hard=False):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    output_dim =logits.shape[1]\n",
    "    y = gumbel_softmax_sample(logits, temperature, gumbel_noise)\n",
    "\n",
    "    if not hard:\n",
    "        ret = y.view(-1, output_dim)\n",
    "        return ret\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    # ret = y_hard.view(-1, output_dim)\n",
    "    ret = y_hard.view(-1, output_dim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "class SampleGenerator(nn.Module):\n",
    "    def __init__(self, input_size, output_size=2):\n",
    "        super(SampleGenerator, self).__init__()\n",
    "        hidden_size=50\n",
    "        # Define learnable latent parameter (no input needed)\n",
    "        self.latent = nn.Parameter(torch.randn(N, hidden_size))\n",
    "        \n",
    "        # if input_size==0:\n",
    "        input_size= input_size+ hidden_size\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc0 = nn.Linear(input_size, hidden_size)  # Hidden layer\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)  # Hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer to predict logits\n",
    "\n",
    "    def forward(self, parents, temperature=1.0, hard=False):\n",
    "        \n",
    "\n",
    "        if parents==None:\n",
    "            parents=self.latent\n",
    "        else:\n",
    "            parents= torch.tensor(parents)\n",
    "            parents= torch.cat([parents, self.latent], dim=1)\n",
    "        \n",
    "        # Use the learnable latent vector\n",
    "        x= torch.relu(self.fc0(parents))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        logits = torch.relu(self.fc2(x))\n",
    "                \n",
    "        \n",
    "        #\n",
    "        # logits = logits / torch.max(logits)  # normalize logits\n",
    "        #\n",
    "        \n",
    "        # Apply Gumbel-Softmax to generate differentiable one-hot like vectors\n",
    "        # gumbel_softmax_output = nn.functional.gumbel_softmax(logits, tau=temperature, hard=hard, eps=1e-10)\n",
    "        \n",
    "        gumbel_softmax_output= gumbel_softmax(logits , temperature, None, False)\n",
    "        \n",
    "        \n",
    "        output=gumbel_softmax_output\n",
    "        if torch.isnan(torch.max(output)):\n",
    "            print('--->',output.shape, ' logists-->', torch.max(logits),torch.min(logits), ' tau-->', temperature)\n",
    "        \n",
    "        \n",
    "        return gumbel_softmax_output\n"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:51:13.836802Z",
     "start_time": "2024-10-25T20:51:13.817910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def marginal(input, freq):\n",
    "    \n",
    "    if input==None:\n",
    "        comb=freq\n",
    "    else:\n",
    "        # inp= torch.argmax(input, dim=1).view(-1,1)\n",
    "        comb= torch.cat([freq, input], dim=1)\n",
    "            \n",
    "    unique_rows, counts = np.unique(np.array(comb), axis=0, return_counts=True)\n",
    "    joint_distribution = counts / len(dataset)\n",
    "    \n",
    "    rdict={}\n",
    "    for row, prob in zip(unique_rows, joint_distribution):\n",
    "        rdict[tuple(row)]= prob\n",
    "        # print(f\"Row: {row}, Probability: {prob:.4f}\")\n",
    "    \n",
    "    return rdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(input, targets, model, name, optimize='maximize', lambdaa=[0.1, 0.1]):\n",
    "\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop (for one sample per forward pass)\n",
    "    for epoch in range(2000):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "        # Forward pass to generate sample\n",
    "        output = model(input, temperature=1.0, hard=False)  # Model generates sample\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = criterion(output, targets) #P(Y|X,Z)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(torch.max(output), torch.min(output))\n",
    "            break\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Causal efffect loss\n",
    "        if name=='Y' and optimize!='none':\n",
    "            if optimize=='maximize':\n",
    "                targAll= torch.nn.functional.one_hot(torch.ones(N).to(torch.int64), num_classes=2).float()\n",
    "            elif optimize=='minimize':\n",
    "                targAll= torch.nn.functional.one_hot(0* torch.ones(N).to(torch.int64), num_classes=2).float()\n",
    "            \n",
    "            # do(X=0) # P(Y=1|do(X=0)) to maximize;\n",
    "            X= torch.nn.functional.one_hot(torch.zeros(N).to(torch.int64), num_classes=2).float()\n",
    "            inputY= torch.cat([sampleZ, X], dim=1)  # X=0, Z~ P(Z) D[Z]\n",
    "            outY0= model(inputY, temperature=1.0, hard=False)    # P(Y|x=0,Z)  of \\sum_z P(z) P(Y|x=0,z)\n",
    "            loss0 = criterion(outY0, targAll) \n",
    "            \n",
    "            # \n",
    "            # 10000 X 2 : outY0: [0.1 , 0.9], [0.8, 0.2] ->  targAll: [0,1], [0,1]\n",
    "            \n",
    "            \n",
    "            # do(X=1)     # P(Y=1|do(X=1)) to maximize;\n",
    "            X= torch.nn.functional.one_hot(torch.ones(N).to(torch.int64), num_classes=2).float()\n",
    "            inputY= torch.cat([sampleZ, X], dim=1)\n",
    "            outY1= model(inputY, temperature=1.0, hard=False)\n",
    "            loss1 = criterion(outY1, targAll)\n",
    "            \n",
    "                        \n",
    "            # if torch.isnan(loss0):\n",
    "            #     print(torch.max(outY0), torch.min(outY0))\n",
    "            #     break\n",
    "            #     \n",
    "            #     \n",
    "            # if torch.isnan(loss1):\n",
    "            #     print(torch.max(outY1), torch.min(outY1))\n",
    "            #     break\n",
    "            \n",
    "                                \n",
    "            # loss+= 0.1*(loss0 + loss1)  # why optimizing both is wrong?  Check Bengio's paper.\n",
    "            loss+= lambdaa[0]*(loss0) + lambdaa[1]*(loss1) \n",
    "            # loss+= lambdaa*(loss1)\n",
    "            \n",
    "        \n",
    "        #\n",
    "        \n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if (epoch+1) % 50 == 0:\n",
    "            \n",
    "            freq= torch.argmax(output, dim=1).view(-1,1)\n",
    "            rdict= marginal(input, freq)\n",
    "            \n",
    "            \n",
    "            freq= torch.argmax(targets, dim=1).view(-1,1)\n",
    "            tdict= marginal(input, freq)\n",
    "                \n",
    "            tvd=0\n",
    "            for key in rdict:\n",
    "                tvd+= abs(rdict[key]- tdict[key])\n",
    "                \n",
    "            \n",
    "            ###\n",
    "            if name=='Y':\n",
    "                X= torch.nn.functional.one_hot(torch.zeros(N).to(torch.int64), num_classes=2).float()\n",
    "                inputY= torch.cat([sampleZ, X], dim=1)\n",
    "                outY= model(inputY, temperature=1.0, hard=False)  \n",
    "                outY= torch.argmax(outY, dim=1).view(-1,1)\n",
    "                doX0= marginal(None, outY)\n",
    "                \n",
    "                newDict={}\n",
    "                for key in doX0:\n",
    "                    newDict[ key+(0,)]  = doX0[key]\n",
    "                doX0= newDict\n",
    "                \n",
    "                # tvdx0=0\n",
    "                # for key in doX0:\n",
    "                #     tvdx0+= abs(doX0[key]- true_doX0[key])\n",
    "                \n",
    "                \n",
    "                ###\n",
    "            \n",
    "                X= torch.nn.functional.one_hot(torch.ones(N).to(torch.int64), num_classes=2).float()\n",
    "                inputY= torch.cat([sampleZ, X], dim=1)\n",
    "                outY= model(inputY, temperature=1.0, hard=False)  \n",
    "                outY= torch.argmax(outY, dim=1).view(-1,1)\n",
    "                doX1= marginal(None, outY)\n",
    "                \n",
    "                newDict={}\n",
    "                for key in doX1:\n",
    "                    newDict[ key+(1,)]  = doX1[key]\n",
    "                doX1= newDict\n",
    "                \n",
    "                \n",
    "                pred_doX= {**doX0, **doX1}\n",
    "                \n",
    "                tvdDox=0\n",
    "                for key in doX1:\n",
    "                    tvdDox+= abs(pred_doX[key]- true_doX[key])\n",
    "                    \n",
    "                \n",
    "                # print(f'Causal effect: P(Y|do(X=0):{doX0} & P(Y|do(X=1):{doX1}')\n",
    "                # print(f'-->{pred_doX} and {true_doX}')\n",
    "            ###\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f} obsTVD:{tvd/2:0.4f} iTVD:{tvdDox/2:0.4f}')\n",
    "            \n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    print('learned', rdict)\n",
    "    print('True', tdict)\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Y"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T21:10:35.195497Z",
     "start_time": "2024-10-25T21:09:54.100946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input= torch.cat([sampleZ, sampleX], dim=1)\n",
    "targets = sampleY\n",
    "input_size=4\n",
    "\n",
    "modelY = SampleGenerator(input_size=input_size)  # P(Y|X,Z)\n",
    "# modelY= train(input, targets, modelY, name=\"Y\", optimize= \"maximize\")\n",
    "modelY= train(input, targets, modelY, name=\"Y\", optimize= \"maximize\", lambdaa= [0.1, 0])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_613967/3770493806.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  parents= torch.tensor(parents)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 0.5252 obsTVD:0.1473 iTVD:0.0007\n",
      "Epoch [100/1000], Loss: 0.5097 obsTVD:0.1640 iTVD:0.0120\n",
      "Epoch [150/1000], Loss: 0.5076 obsTVD:0.1627 iTVD:0.0112\n",
      "Epoch [200/1000], Loss: 0.4975 obsTVD:0.1531 iTVD:0.0057\n",
      "Epoch [250/1000], Loss: 0.4812 obsTVD:0.1365 iTVD:0.0108\n",
      "Epoch [300/1000], Loss: 0.4627 obsTVD:0.1198 iTVD:0.0261\n",
      "Epoch [350/1000], Loss: 0.4454 obsTVD:0.1038 iTVD:0.0382\n",
      "Epoch [400/1000], Loss: 0.4396 obsTVD:0.0989 iTVD:0.0463\n",
      "Epoch [450/1000], Loss: 0.4371 obsTVD:0.0966 iTVD:0.0485\n",
      "Epoch [500/1000], Loss: 0.4373 obsTVD:0.0956 iTVD:0.0488\n",
      "Epoch [550/1000], Loss: 0.4372 obsTVD:0.1009 iTVD:0.0499\n",
      "Epoch [600/1000], Loss: 0.4353 obsTVD:0.0951 iTVD:0.0498\n",
      "Epoch [650/1000], Loss: 0.4361 obsTVD:0.0955 iTVD:0.0516\n",
      "Epoch [700/1000], Loss: 0.4355 obsTVD:0.0955 iTVD:0.0510\n",
      "Epoch [750/1000], Loss: 0.4339 obsTVD:0.0939 iTVD:0.0486\n",
      "Epoch [800/1000], Loss: 0.4337 obsTVD:0.0927 iTVD:0.0515\n",
      "Epoch [850/1000], Loss: 0.4341 obsTVD:0.0948 iTVD:0.0534\n",
      "Epoch [900/1000], Loss: 0.4335 obsTVD:0.0922 iTVD:0.0521\n",
      "Epoch [950/1000], Loss: 0.4343 obsTVD:0.0955 iTVD:0.0500\n",
      "Epoch [1000/1000], Loss: 0.4340 obsTVD:0.0960 iTVD:0.0521\n",
      "Epoch [1050/1000], Loss: 0.4335 obsTVD:0.0942 iTVD:0.0506\n",
      "Epoch [1100/1000], Loss: 0.4321 obsTVD:0.0928 iTVD:0.0527\n",
      "Epoch [1150/1000], Loss: 0.4340 obsTVD:0.0952 iTVD:0.0531\n",
      "Epoch [1200/1000], Loss: 0.4360 obsTVD:0.0973 iTVD:0.0495\n",
      "Epoch [1250/1000], Loss: 0.4335 obsTVD:0.0939 iTVD:0.0502\n",
      "Epoch [1300/1000], Loss: 0.4317 obsTVD:0.0894 iTVD:0.0489\n",
      "Epoch [1350/1000], Loss: 0.4336 obsTVD:0.0942 iTVD:0.0503\n",
      "Epoch [1400/1000], Loss: 0.4326 obsTVD:0.0933 iTVD:0.0491\n",
      "Epoch [1450/1000], Loss: 0.4333 obsTVD:0.0945 iTVD:0.0520\n",
      "Epoch [1500/1000], Loss: 0.4332 obsTVD:0.0923 iTVD:0.0497\n",
      "Epoch [1550/1000], Loss: 0.4329 obsTVD:0.0944 iTVD:0.0492\n",
      "Epoch [1600/1000], Loss: 0.4330 obsTVD:0.0920 iTVD:0.0503\n",
      "Epoch [1650/1000], Loss: 0.4332 obsTVD:0.0936 iTVD:0.0495\n",
      "Epoch [1700/1000], Loss: 0.4332 obsTVD:0.0936 iTVD:0.0487\n",
      "Epoch [1750/1000], Loss: 0.4351 obsTVD:0.0958 iTVD:0.0504\n",
      "Epoch [1800/1000], Loss: 0.4334 obsTVD:0.0929 iTVD:0.0512\n",
      "Epoch [1850/1000], Loss: 0.4331 obsTVD:0.0938 iTVD:0.0494\n",
      "Epoch [1900/1000], Loss: 0.4338 obsTVD:0.0955 iTVD:0.0490\n",
      "Epoch [1950/1000], Loss: 0.4322 obsTVD:0.0944 iTVD:0.0510\n",
      "Epoch [2000/1000], Loss: 0.4334 obsTVD:0.0933 iTVD:0.0488\n",
      "Training complete!\n",
      "learned {(0.0, 0.0, 1.0, 0.0, 1.0): 0.0024, (0.0, 0.0, 1.0, 1.0, 0.0): 0.0221, (0.0, 1.0, 0.0, 1.0, 0.0): 0.0473, (1.0, 0.0, 1.0, 0.0, 1.0): 0.4213, (1.0, 0.0, 1.0, 1.0, 0.0): 0.1683, (1.0, 1.0, 0.0, 0.0, 1.0): 0.0086, (1.0, 1.0, 0.0, 1.0, 0.0): 0.33}\n",
      "True {(0.0, 0.0, 1.0, 0.0, 1.0): 0.0067, (0.0, 0.0, 1.0, 1.0, 0.0): 0.0523, (0.0, 1.0, 0.0, 0.0, 1.0): 0.0004, (0.0, 1.0, 0.0, 1.0, 0.0): 0.1059, (1.0, 0.0, 1.0, 0.0, 1.0): 0.417, (1.0, 0.0, 1.0, 1.0, 0.0): 0.1381, (1.0, 1.0, 0.0, 0.0, 1.0): 0.0082, (1.0, 1.0, 0.0, 1.0, 0.0): 0.2714}\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input= torch.cat([sampleZ, sampleX], dim=1)\n",
    "targets = sampleY\n",
    "input_size=4\n",
    "\n",
    "modelY = SampleGenerator(input_size=input_size)  # P(Y|X,Z)\n",
    "modelY= train(input, targets, modelY, name=\"Y\", optimize= \"minimize\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Appendix"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# input= torch.cat([sampleZ, sampleX], dim=1)\n",
    "input= None\n",
    "targets = sampleZ\n",
    "input_size=0\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "modelZ = SampleGenerator(input_size=input_size)\n",
    "\n",
    "modelZ= train(input, targets, modelZ, name=\"Z\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# input= torch.cat([sampleZ, sampleX], dim=1)\n",
    "input= sampleZ\n",
    "targets = sampleX\n",
    "input_size=2\n",
    "\n",
    "modelX = SampleGenerator(input_size=input_size)\n",
    "\n",
    "modelX= train(input, targets, modelX, name='X')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outZ = modelZ(None, temperature=1.0, hard=True)\n",
    "outt= outZ\n",
    "\n",
    "outt= torch.argmax(outt, dim=1).view(-1,1)\n",
    "unique_rows, counts = np.unique(np.array(outt.detach()), axis=0, return_counts=True)\n",
    "joint_distribution = counts / len(outt)\n",
    "joint_distribution"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X= torch.ones_like(sampleZ)\n",
    "inputY= torch.cat([sampleZ, X], dim=1)\n",
    "outY= modelY(inputY, temperature=1.0, hard=False)  \n",
    "\n",
    "\n",
    "outt= outY\n",
    "outt= torch.argmax(outt, dim=1).view(-1,1)\n",
    "unique_rows, counts = np.unique(np.array(outt.detach()), axis=0, return_counts=True)\n",
    "joint_distribution = counts / len(outt)\n",
    "joint_distribution"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
